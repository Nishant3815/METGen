{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Evaluation_Classification_Model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXppV_hcATME","executionInfo":{"status":"ok","timestamp":1639204949203,"user_tz":300,"elapsed":15558,"user":{"displayName":"Nishant Raj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05136613897994739814"}},"outputId":"308bf613-c9b3-42a7-9a95-c076e23511cd"},"source":["# Code to mount the drive \n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoUzvZPpAh0g","executionInfo":{"status":"ok","timestamp":1638921495458,"user_tz":300,"elapsed":241,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"348889a5-2305-4533-9827-26ff3915a53f"},"source":["%cd '/content/drive/MyDrive/685/catr/'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/12c1zkm0_oa8VcOfsUa8Tn_YcYdyPpSlP/685/catr\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZe8sYGePueO","executionInfo":{"status":"ok","timestamp":1638921507450,"user_tz":300,"elapsed":8457,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"44c38ea5-4ba2-4632-ed3e-5d4300e75203"},"source":["!pip install -U sentence-transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 4.9 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 17.6 MB/s \n","\u001b[?25hCollecting tokenizers>=0.10.3\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 57.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 49.1 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 420 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 60.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 66.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=0ac32b5861479ccf7b2f57260644cf00a81b56d0ee008afa1c33b32b885b3a9f\n","  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n","Successfully built sentence-transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxaKzBrjAo9E","executionInfo":{"status":"ok","timestamp":1638921510359,"user_tz":300,"elapsed":2923,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"5487127a-7fab-404a-ad2c-0c8183bd4735"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.12.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (3.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (4.8.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.2.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.0.46)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (2019.12.20)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r requirements.txt (line 4)) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 4)) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"id":"v6yBGCRlB6Dg"},"source":["from transformers import ViTModel, ViTConfig, ViTFeatureExtractor,BertTokenizer,BertForMaskedLM\n","from sentence_transformers import SentenceTransformer\n","import torch\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import auc, precision_score, recall_score,roc_auc_score\n","import xgboost as xgb\n","from PIL import Image\n","import argparse\n","import glob\n","import json\n","import pandas as pd\n","import numpy as np\n","from models import caption\n","from datasets import coco, utils\n","from tqdm import tqdm\n","from configuration import Config\n","from xgboost import XGBClassifier\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sa9Wwjec2obu"},"source":["def extract_vision_transformer_feats(image_path,n_select=None):\n","  \"\"\"\n","  Function to extract the features from vision transformers\n","  \"\"\"\n","  if not n_select:\n","    img_files_list = glob.glob(image_path+\"*\")\n","  else:\n","    img_files_list = glob.glob(image_path+\"*\")[:n_select]\n","  \n","  # Create image batch array for Vision Transformer\n","  img_batch = []\n","  for file in tqdm(img_files_list):\n","    img = np.asarray(Image.open(file))\n","    img_batch.append(img)\n","  print(\"Creation of image batches to be used for Vision Transformer complete\")\n","\n","  # Extract features from ViTModel\n","  feature_extractor   = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n","  model               = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","  inputs              = feature_extractor(images=img_batch, return_tensors=\"pt\")\n","  outputs             = model(**inputs)\n","\n","  # Get image representations and their corresponding hashes i.e. get [CLS] token representation for each image\n","  img_representations = outputs.last_hidden_state[:,0,:]\n","  img_hash_li         = []\n","  for file in img_files_list:\n","    img_hash_li.append(file.split(\"/\")[-1][:-4])\n","  \n","  # Create column names for image dimensions \n","  col_img  = [\"imdim_\"+str(i) for i in list(range(768))]\n","\n","  # Create a dataframe of image features\n","  img_data = pd.DataFrame(img_representations.detach().numpy(),columns = col_img)\n","  img_data['img_hash'] = img_hash_li\n","\n","  return img_data\n","\n","def extract_sentence_transformer_feats(reference_file_pth = '../emogen/annotations/',json_file = 'captions_train.json'):\n","  \"\"\"\n","  Extract features for emotion related texts\n","  \"\"\"\n","  # Get reference text data\n","  combined_path = reference_file_pth + json_file\n","  file = open(combined_path)\n","  data_json = json.load(file)\n","  data = pd.DataFrame.from_dict(data_json['annotations']).reset_index(drop=True)\n","  try:\n","    del data['id']\n","  except:\n","    pass\n","  \n","  # Use Sentence Transformer to extract features\n","  model = SentenceTransformer('all-mpnet-base-v2')\n","  sentence_embeddings = model.encode(data['caption'])\n","  col_text = [\"tdim_\"+str(i) for i in list(range(768))]\n","\n","  # Create text feature dataframe\n","  text_data = pd.DataFrame(sentence_embeddings,columns=col_text)\n","  text_data['img_hash'] = data['image_id']\n","\n","  return text_data\n","\n","def xgb_train_kfold(X_trn, y_trn,n_splits=5):\n","  \"\"\"\n","  Perform training with XGBoost and evaluate in K-Fold cross-validation settings \n","  \"\"\"\n","  errors    = []\n","  precision = []\n","  recall    = []\n","  auc       = []\n","  kf = KFold(n_splits=n_splits, shuffle=True, random_state=3815)\n","\n","  for train_index, test_index in kf.split(X_trn):\n","     X_train_n, X_test_n = X_trn.values[train_index], X_trn.values[test_index]\n","     y_train_n, y_test_n = y_trn.values[train_index], y_trn.values[test_index]\n","\n","     model = XGBClassifier(\n","         max_depth=4, n_estimators=100, random_state = 3815\n","     )\n","     model.fit(X_train_n, y_train_n)\n","     y_pred = model.predict(X_test_n)\n","     accuracy = (sum(y_pred == y_test_n))/len(y_test_n)\n","     errors.append(1 - accuracy)\n","     precision.append(precision_score(y_test_n,y_pred))\n","     recall.append(recall_score(y_test_n,y_pred))\n","     auc.append(roc_auc_score(y_test_n,y_pred))\n","\n","  return errors, precision, recall, auc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daKidWVgGQWj","executionInfo":{"status":"ok","timestamp":1638884484423,"user_tz":300,"elapsed":20967,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"18097639-8f58-48a5-c7a3-75c6f21d23e7"},"source":["img_data = extract_vision_transformer_feats(image_path= '../emogen/train/',n_select=75)\n","text_data = extract_sentence_transformer_feats(reference_file_pth = '../emogen/annotations/',json_file = 'captions_train.json')\n","# Concat data to for final dataset with multimodal features  \n","mmodal_data = img_data.merge(text_data,how='left',on='img_hash')\n","# Create pseudo labels for interim \n","pseudo_labels = [np.random.randint(0,2) for i in range(len(mmodal_data))]\n","mmodal_data['Target']  = pseudo_labels\n","###########################################################\n","y_trn = mmodal_data['Target']\n","X_trn = mmodal_data.drop(columns=['Target','img_hash'],axis=1)\n","errors, precision, recall, auc = xgb_train_kfold(X_trn,y_trn)\n","# Check average performances across K-Folds\n","print(\"The training errors on average is: \", np.round(np.mean(errors),4)*100)\n","print(\"The Precision on average is: \", np.round(np.mean(precision),4)*100)\n","print(\"The Recall on average is: \", np.round(np.mean(recall),4)*100)\n","print(\"The AUC Score on average is: \", np.round(np.mean(auc),4)*100)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The training errors on average is:  60.0\n","The Precision on average is:  36.69\n","The Recall on average is:  34.33\n","The AUC Score on average is:  42.75\n"]}]},{"cell_type":"code","metadata":{"id":"uLS1Fsg8G_Us"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpvQ47uGHmfb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mn6fS1teHrI-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzRwwgbQHrFY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lb4hGobFEOgn"},"source":["# def xgb_train_kfold(X_trn, y_trn,n_splits=5):\n","#   \"\"\"\n","#   Perform training with XGBoost and evaluate in K-Fold cross-validation settings \n","#   \"\"\"\n","#   errors    = []\n","#   precision = []\n","#   recall    = []\n","#   auc       = []\n","#   kf = KFold(n_splits=n_splits, shuffle=True, random_state=3815)\n","\n","#   for train_index, test_index in kf.split(X_trn):\n","#      X_train_n, X_test_n = X_trn.values[train_index], X_trn.values[test_index]\n","#      y_train_n, y_test_n = y_trn.values[train_index], y_trn.values[test_index]\n","\n","#      model = XGBClassifier(\n","#          max_depth=4, n_estimators=100, random_state = 3815\n","#      )\n","#      model.fit(X_train_n, y_train_n)\n","#      y_pred = model.predict(X_test_n)\n","#      accuracy = (sum(y_pred == y_test_n))/len(y_test_n)\n","#      errors.append(1 - accuracy)\n","#      precision.append(precision_score(y_test_n,y_pred))\n","#      recall.append(recall_score(y_test_n,y_pred))\n","#      auc.append(roc_auc_score(y_test_n,y_pred))\n","\n","#   return errors, precision, recall, auc\n","\n","\n","# Assign image folder, image path and mmodel checkpoint path \n","# image_path = '../emogen/train/'\n","# #img_name   = '0c4ff62871a904274fd41cee695d85f.jpg'\n","# #img_final_loc = image_path+img_name\n","# # Load image and perform pre-processing \n","# #image = Image.open(img_final_loc)\n","\n","# img_files_list = glob.glob(image_path+\"*\")[:50] # For test run \n","\n","# img_batch = []\n","# for file in tqdm(img_files_list):\n","#   img = np.asarray(Image.open(file))\n","#   img_batch.append(img)\n","\n","# # Extract features\n","# feature_extractor   = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n","# model               = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","# inputs              = feature_extractor(images=img_batch, return_tensors=\"pt\")\n","# outputs             = model(**inputs)\n","\n","# # Get image representations and their corresponding hashes \n","# img_representations = outputs.last_hidden_state[:,0,:]\n","# img_hash_li         = []\n","# for file in img_files_list:\n","#   img_hash_li.append(file.split(\"/\")[-1][:-4])\n","\n","# # Get reference text data\n","# reference_file_pth = '../emogen/annotations/'\n","# json_file = 'captions_train.json'\n","# combined_path = reference_file_pth + json_file\n","\n","# file = open(combined_path)\n","# data_json = json.load(file)\n","# data = pd.DataFrame.from_dict(data_json['annotations']).reset_index(drop=True)\n","# try:\n","#   del data['id']\n","# except:\n","#   pass\n","# model = SentenceTransformer('all-mpnet-base-v2')\n","# sentence_embeddings = model.encode(data['caption'])\n","\n","# # Create column names \n","# col_text = [\"tdim_\"+str(i) for i in list(range(768))]\n","# col_img  = [\"imdim_\"+str(i) for i in list(range(768))]\n","# # Create DataFrame for text features  \n","# text_data = pd.DataFrame(sentence_embeddings,columns=col_text)\n","# text_data['img_hash'] = data['image_id']\n","# # Create a dataframe of image features\n","# img_data = pd.DataFrame(img_representations.detach().numpy(),columns = col_img)\n","# img_data['img_hash'] = img_hash_li\n","\n","# # Concat data to for final dataset with multimodal features  \n","# mmodal_data = img_data.merge(text_data,how='left',on='img_hash')\n","\n","# # Create pseudo labels for interim \n","# pseudo_labels = [np.random.randint(0,2) for i in range(len(mmodal_data))]\n","# mmodal_data['Target']  = pseudo_labels\n","\n","# y_trn = mmodal_data['Target']\n","# X_trn = mmodal_data.drop(columns=['Target','img_hash'],axis=1)\n","\n","# errors, precision, recall, auc = xgb_train_kfold(X_trn,y_trn)\n","\n","# # Check average performances across K-Folds\n","# print(\"The training errors on average is: \", np.round(np.mean(errors),4)*100)\n","# print(\"The Precision on average is: \", np.round(np.mean(precision),4)*100)\n","# print(\"The Recall on average is: \", np.round(np.mean(recall),4)*100)\n","# print(\"The AUC Score on average is: \", np.round(np.mean(auc),4)*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmuOyHhPB_GY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiVb9jt26Xha"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMHOLoOhFOwa"},"source":[""],"execution_count":null,"outputs":[]}]}