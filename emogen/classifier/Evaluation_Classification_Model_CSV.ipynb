{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Evaluation_Classification_Model_CSV.ipynb","provenance":[{"file_id":"1nmds2EFGSqlh5lheyWkJgGMTr-t7HThB","timestamp":1638921776441}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXppV_hcATME","executionInfo":{"status":"ok","timestamp":1638997582787,"user_tz":300,"elapsed":21944,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"db93cac9-c900-40e8-8ace-dbb5ccbec513"},"source":["# Code to mount the drive \n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoUzvZPpAh0g","executionInfo":{"status":"ok","timestamp":1638997583237,"user_tz":300,"elapsed":454,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"59ae8a47-3f99-44f4-c1cd-e88347405ab5"},"source":["%cd '/content/drive/MyDrive/685/catr/'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/12c1zkm0_oa8VcOfsUa8Tn_YcYdyPpSlP/685/catr\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZe8sYGePueO","executionInfo":{"status":"ok","timestamp":1638997592122,"user_tz":300,"elapsed":8293,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"7d067d9f-3187-4356-d36e-9148eb9f2110"},"source":["!pip install -U sentence-transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 5.0 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 16.9 MB/s \n","\u001b[?25hCollecting tokenizers>=0.10.3\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 92.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 65.4 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 608 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 62.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 63.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=9bbcd487252ed1c6ff5fb7efee1958f0b8fcc3fa6dbcd111ca2edd31bf39b0bc\n","  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n","Successfully built sentence-transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxaKzBrjAo9E","executionInfo":{"status":"ok","timestamp":1638997595108,"user_tz":300,"elapsed":2991,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"259abcc5-19c4-4739-b580-5a11d27153f4"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.12.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.0.46)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.10.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r requirements.txt (line 4)) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 4)) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"id":"v6yBGCRlB6Dg"},"source":["from transformers import ViTModel, ViTConfig, ViTFeatureExtractor,BertTokenizer,BertForMaskedLM\n","from sentence_transformers import SentenceTransformer\n","import torch\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import auc, precision_score, recall_score,roc_auc_score\n","import xgboost as xgb\n","from PIL import Image\n","import argparse\n","import glob\n","import json\n","import pandas as pd\n","import numpy as np\n","from models import caption\n","from datasets import coco, utils\n","from tqdm import tqdm\n","from configuration import Config\n","from xgboost import XGBClassifier\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sa9Wwjec2obu"},"source":["def extract_vision_transformer_feats(image_path,n_select=None):\n","  \"\"\"\n","  Function to extract the features from vision transformers\n","  \"\"\"\n","  if not n_select:\n","    img_files_list = glob.glob(image_path+\"*\")\n","  else:\n","    img_files_list = glob.glob(image_path+\"*\")[:n_select]\n","  \n","  # Create image batch array for Vision Transformer\n","  img_batch = []\n","  for file in tqdm(img_files_list):\n","    img = np.asarray(Image.open(file))\n","    img_batch.append(img)\n","  print(\"Creation of image batches to be used for Vision Transformer complete\")\n","\n","  # Extract features from ViTModel\n","  feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-small-distilled-patch16-224')\n","  model = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-small-distilled-patch16-224')\n","  inputs              = feature_extractor(images=img_batch, return_tensors=\"pt\")\n","  outputs             = model(**inputs)\n","\n","  # Get image representations and their corresponding hashes i.e. get [CLS] token representation for each image\n","  img_representations = outputs.last_hidden_state[:,0,:]\n","  img_hash_li         = []\n","  for file in img_files_list:\n","    img_hash_li.append(file.split(\"/\")[-1][:-4])\n","  \n","  # Create column names for image dimensions \n","  col_img  = [\"imdim_\"+str(i) for i in list(range(768))]\n","\n","  # Create a dataframe of image features\n","  img_data = pd.DataFrame(img_representations.detach().numpy(),columns = col_img)\n","  img_data['img_hash'] = img_hash_li\n","\n","  return img_data\n","\n","def extract_sentence_transformer_feats(reference_file_pth = '../emogen/Classifier/train/caption/',csv_file = 'train_caption.csv'):\n","  \"\"\"\n","  Extract features for emotion related texts\n","  \"\"\"\n","  # Get reference text data\n","  combined_path = reference_file_pth + csv_file\n","  #file = open(combined_path)\n","  #data_json = json.load(file)\n","  data = pd.read_csv(combined_path)#pd.DataFrame.from_dict(data_json['annotations']).reset_index(drop=True)\n","  try:\n","    del data['Unnamed: 0']\n","  except:\n","    pass\n","  \n","  # Use Sentence Transformer to extract features\n","  model = SentenceTransformer('all-mpnet-base-v2')\n","  sentence_embeddings = model.encode(data['comment'])\n","  col_text = [\"tdim_\"+str(i) for i in list(range(768))]\n","\n","  # Create text feature dataframe\n","  text_data = pd.DataFrame(sentence_embeddings,columns=col_text)\n","  text_data['img_hash'] = data['image_hash']\n","  text_data['label'] = data['label']\n","\n","  return text_data\n","\n","def xgb_train_kfold(X_trn, y_trn,n_splits=5):\n","  \"\"\"\n","  Perform training with XGBoost and evaluate in K-Fold cross-validation settings \n","  \"\"\"\n","  errors    = []\n","  precision = []\n","  recall    = []\n","  auc       = []\n","  kf = KFold(n_splits=n_splits, shuffle=True, random_state=3815)\n","\n","  for train_index, test_index in kf.split(X_trn):\n","     X_train_n, X_test_n = X_trn.values[train_index], X_trn.values[test_index]\n","     y_train_n, y_test_n = y_trn.values[train_index], y_trn.values[test_index]\n","\n","     model = XGBClassifier(\n","         max_depth=4, n_estimators=300, random_state = 3815\n","     )\n","     model.fit(X_train_n, y_train_n)\n","     y_pred = model.predict(X_test_n)\n","     accuracy = (sum(y_pred == y_test_n))/len(y_test_n)\n","     errors.append(1 - accuracy)\n","     precision.append(precision_score(y_test_n,y_pred))\n","     recall.append(recall_score(y_test_n,y_pred))\n","     auc.append(roc_auc_score(y_test_n,y_pred))\n","\n","  return errors, precision, recall, auc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":894},"id":"daKidWVgGQWj","executionInfo":{"status":"error","timestamp":1638997777851,"user_tz":300,"elapsed":151533,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"b93f32fa-3513-4b00-a962-e75c35e8e327"},"source":["img_data = extract_vision_transformer_feats(image_path= '../emogen/Classifier/train/images/',n_select=1250)\n","text_data = extract_sentence_transformer_feats()\n","# Concat data to for final dataset with multimodal features  \n","mmodal_data = img_data.merge(text_data,how='outer',on='img_hash')\n","#print(mmodal_data.columns)\n","labels = [int(x) for x in mmodal_data['label']]\n","# Create pseudo labels for interim \n","#pseudo_labels = [np.random.randint(0,2) for i in range(len(mmodal_data))]\n","mmodal_data['Target']  = labels\n","###########################################################\n","y_trn = mmodal_data['Target']\n","X_trn = mmodal_data.drop(columns=['label','Target','img_hash'],axis=1)\n","errors, precision, recall, auc = xgb_train_kfold(X_trn,y_trn)\n","# Check average performances across K-Folds\n","print(\"The training errors on average is: \", np.round(np.mean(errors),4)*100)\n","print(\"The Precision on average is: \", np.round(np.mean(precision),4)*100)\n","print(\"The Recall on average is: \", np.round(np.mean(recall),4)*100)\n","print(\"The AUC Score on average is: \", np.round(np.mean(auc),4)*100)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  6%|▌         | 76/1250 [01:18<20:11,  1.03s/it]  \n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-7-a583aaaaac62>\", line 1, in <module>\n","    img_data = extract_vision_transformer_feats(image_path= '../emogen/Classifier/train/images/',n_select=1250)\n","  File \"<ipython-input-6-91504a7a5b2a>\", line 13, in extract_vision_transformer_feats\n","    img = np.asarray(Image.open(file))\n","  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2852, in open\n","    prefix = fp.read(16)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/genericpath.py\", line 19, in exists\n","    os.stat(path)\n","FileNotFoundError: [Errno 2] No such file or directory: '<ipython-input-7-a583aaaaac62>'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.7/inspect.py\", line 693, in getsourcefile\n","    if os.path.exists(filename):\n","  File \"/usr/lib/python3.7/genericpath.py\", line 19, in exists\n","    os.stat(path)\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"uLS1Fsg8G_Us"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpvQ47uGHmfb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mn6fS1teHrI-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzRwwgbQHrFY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lb4hGobFEOgn"},"source":["# def xgb_train_kfold(X_trn, y_trn,n_splits=5):\n","#   \"\"\"\n","#   Perform training with XGBoost and evaluate in K-Fold cross-validation settings \n","#   \"\"\"\n","#   errors    = []\n","#   precision = []\n","#   recall    = []\n","#   auc       = []\n","#   kf = KFold(n_splits=n_splits, shuffle=True, random_state=3815)\n","\n","#   for train_index, test_index in kf.split(X_trn):\n","#      X_train_n, X_test_n = X_trn.values[train_index], X_trn.values[test_index]\n","#      y_train_n, y_test_n = y_trn.values[train_index], y_trn.values[test_index]\n","\n","#      model = XGBClassifier(\n","#          max_depth=4, n_estimators=100, random_state = 3815\n","#      )\n","#      model.fit(X_train_n, y_train_n)\n","#      y_pred = model.predict(X_test_n)\n","#      accuracy = (sum(y_pred == y_test_n))/len(y_test_n)\n","#      errors.append(1 - accuracy)\n","#      precision.append(precision_score(y_test_n,y_pred))\n","#      recall.append(recall_score(y_test_n,y_pred))\n","#      auc.append(roc_auc_score(y_test_n,y_pred))\n","\n","#   return errors, precision, recall, auc\n","\n","\n","# Assign image folder, image path and mmodel checkpoint path \n","# image_path = '../emogen/train/'\n","# #img_name   = '0c4ff62871a904274fd41cee695d85f.jpg'\n","# #img_final_loc = image_path+img_name\n","# # Load image and perform pre-processing \n","# #image = Image.open(img_final_loc)\n","\n","# img_files_list = glob.glob(image_path+\"*\")[:50] # For test run \n","\n","# img_batch = []\n","# for file in tqdm(img_files_list):\n","#   img = np.asarray(Image.open(file))\n","#   img_batch.append(img)\n","\n","# # Extract features\n","# feature_extractor   = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n","# model               = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","# inputs              = feature_extractor(images=img_batch, return_tensors=\"pt\")\n","# outputs             = model(**inputs)\n","\n","# # Get image representations and their corresponding hashes \n","# img_representations = outputs.last_hidden_state[:,0,:]\n","# img_hash_li         = []\n","# for file in img_files_list:\n","#   img_hash_li.append(file.split(\"/\")[-1][:-4])\n","\n","# # Get reference text data\n","# reference_file_pth = '../emogen/annotations/'\n","# json_file = 'captions_train.json'\n","# combined_path = reference_file_pth + json_file\n","\n","# file = open(combined_path)\n","# data_json = json.load(file)\n","# data = pd.DataFrame.from_dict(data_json['annotations']).reset_index(drop=True)\n","# try:\n","#   del data['id']\n","# except:\n","#   pass\n","# model = SentenceTransformer('all-mpnet-base-v2')\n","# sentence_embeddings = model.encode(data['caption'])\n","\n","# # Create column names \n","# col_text = [\"tdim_\"+str(i) for i in list(range(768))]\n","# col_img  = [\"imdim_\"+str(i) for i in list(range(768))]\n","# # Create DataFrame for text features  \n","# text_data = pd.DataFrame(sentence_embeddings,columns=col_text)\n","# text_data['img_hash'] = data['image_id']\n","# # Create a dataframe of image features\n","# img_data = pd.DataFrame(img_representations.detach().numpy(),columns = col_img)\n","# img_data['img_hash'] = img_hash_li\n","\n","# # Concat data to for final dataset with multimodal features  \n","# mmodal_data = img_data.merge(text_data,how='left',on='img_hash')\n","\n","# # Create pseudo labels for interim \n","# pseudo_labels = [np.random.randint(0,2) for i in range(len(mmodal_data))]\n","# mmodal_data['Target']  = pseudo_labels\n","\n","# y_trn = mmodal_data['Target']\n","# X_trn = mmodal_data.drop(columns=['Target','img_hash'],axis=1)\n","\n","# errors, precision, recall, auc = xgb_train_kfold(X_trn,y_trn)\n","\n","# # Check average performances across K-Folds\n","# print(\"The training errors on average is: \", np.round(np.mean(errors),4)*100)\n","# print(\"The Precision on average is: \", np.round(np.mean(precision),4)*100)\n","# print(\"The Recall on average is: \", np.round(np.mean(recall),4)*100)\n","# print(\"The AUC Score on average is: \", np.round(np.mean(auc),4)*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmuOyHhPB_GY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiVb9jt26Xha"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMHOLoOhFOwa"},"source":[""],"execution_count":null,"outputs":[]}]}