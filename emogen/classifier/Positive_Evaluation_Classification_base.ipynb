{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Positive_Evaluation_Classification_base.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rteLtuO1l8Ak","executionInfo":{"status":"ok","timestamp":1639624170719,"user_tz":300,"elapsed":15332,"user":{"displayName":"Himanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02796908766945347166"}},"outputId":"0338ac80-4050-44f5-a0b9-318824883ad9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Code to mount the drive \n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["!pip install -U sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qf9XUofFmIIl","executionInfo":{"status":"ok","timestamp":1639624183369,"user_tz":300,"elapsed":12656,"user":{"displayName":"Himanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02796908766945347166"}},"outputId":"8f5f31d0-5b8d-45d9-cbb5-c5cd8e16a67b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 3.2 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 11.6 MB/s \n","\u001b[?25hCollecting tokenizers>=0.10.3\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 33.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 44.6 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 389 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 26.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 28.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=aba70021b36ef27139e8daa755d2acd5d0b270ab5ecc09b4d62a97b74834f114\n","  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n","Successfully built sentence-transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.14.1\n"]}]},{"cell_type":"code","source":["# cd '/content/drive/MyDrive/685/catr'\n","#For Himanshu's access\n","import os\n","path = \"/content/drive/MyDrive/CollabProjects/685/catr\"\n","os.chdir(path)"],"metadata":{"id":"koeVSgQzmH-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqUQHzdXmS8j","executionInfo":{"status":"ok","timestamp":1639624188214,"user_tz":300,"elapsed":4683,"user":{"displayName":"Himanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02796908766945347166"}},"outputId":"b13f15a4-19f9-49a3-9610-e4103a9edc83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.14.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (4.8.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.2.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.0.46)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (3.4.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 4)) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r requirements.txt (line 4)) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 4)) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 4)) (1.15.0)\n"]}]},{"cell_type":"code","source":["# from transformers import ViTModel, ViTConfig, ViTFeatureExtractor,BertTokenizer,BertForMaskedLM\n","from transformers import DeiTFeatureExtractor, DeiTModel #AutoFeatureExtractor, DeiTForImageClassificationWithTeacher, \n","from sentence_transformers import SentenceTransformer\n","import torch\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import auc, precision_score, recall_score,roc_auc_score\n","import xgboost as xgb\n","from PIL import Image\n","import argparse\n","import glob\n","import json\n","import pandas as pd\n","import numpy as np\n","from models import caption\n","from datasets import coco, utils\n","from tqdm import tqdm\n","from configuration import Config\n","from xgboost import XGBClassifier\n","import os"],"metadata":{"id":"IbLkeXOAmXmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_vision_transformer_feats(image_path,nsel_st=None,nsel_end=None):\n","  \"\"\"\n","  Function to extract the features from vision transformers\n","  \"\"\"\n","  #code to get all images present in a certain csv file\n","  csv_data=pd.read_csv(\"/content/drive/MyDrive/685/emogen/Classifier/train/caption/negative_captions.csv\")\n","  img_list = [os.path.join(image_path,str(i+'.jpg')) for i in list(csv_data['image_hash'])]\n","\n","  if not nsel_st:\n","    img_files_list = img_list#glob.glob(image_path+\"*\")\n","  else:\n","    img_files_list = img_list[nsel_st:nsel_end]#glob.glob(image_path+\"*\")[nsel_st:nsel_end]\n","  \n","  # Create image batch array for Vision Transformer\n","  img_batch = []\n","  for file in tqdm(img_files_list):\n","    img = np.asarray(Image.open(file))\n","    newsize = (240, 240, 3)\n","    img = np.resize(img,newsize)\n","    img_batch.append(img)\n","  print(\"Creation of image batches to be used for Vision Transformer complete\")\n","\n","  \n","  # Extract features from ViTModel\n","  # feature_extractor   = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n","  # model               = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","  feature_extractor   =  DeiTFeatureExtractor.from_pretrained('facebook/deit-tiny-distilled-patch16-224')\n","  model               = DeiTModel.from_pretrained('facebook/deit-tiny-distilled-patch16-224')\n","  inputs              = feature_extractor(images=img_batch, return_tensors=\"pt\")\n","  outputs             = model(**inputs)\n","\n","  # Get image representations and their corresponding hashes i.e. get [CLS] token representation for each image\n","  img_representations = outputs.last_hidden_state[:,0,:]\n","  img_hash_li         = []\n","  for file in img_files_list:\n","    img_hash_li.append(file.split(\"/\")[-1][:-4])\n","  \n","  # Create column names for image dimensions \n","  col_img  = [\"imdim_\"+str(i) for i in list(range(192))]\n","\n","  # Create a dataframe of image features\n","  img_data = pd.DataFrame(img_representations.detach().numpy(),columns = col_img)\n","  img_data['img_hash'] = img_hash_li\n","\n","  return img_data\n","\n","def extract_sentence_transformer_feats(reference_file_pth = '/content/drive/MyDrive/685/emogen/Classifier/train/caption/',csv_file = 'negative_captions.csv',lab_assign=0):\n","  \"\"\"\n","  Extract features for emotion related texts\n","  \"\"\"\n","  # Get reference text data\n","  combined_path = reference_file_pth + csv_file\n","  #file = open(combined_path)\n","  #data_json = json.load(file)\n","  data = pd.read_csv(combined_path)#pd.DataFrame.from_dict(data_json['annotations']).reset_index(drop=True)\n","  data = data[1:2001].reset_index(drop=True)\n","  try:\n","    del data['Unnamed: 0']\n","  except:\n","    pass\n","  \n","  # Use Sentence Transformer to extract features\n","  model = SentenceTransformer('all-mpnet-base-v2')\n","  sentence_embeddings = model.encode(data['comment'])\n","  col_text = [\"tdim_\"+str(i) for i in list(range(768))]\n","\n","  # Create text feature dataframe\n","  text_data = pd.DataFrame(sentence_embeddings,columns=col_text)\n","  text_data['img_hash'] = data['image_hash']\n","  text_data['label'] = lab_assign\n","\n","  return text_data\n","\n","def xgb_train_kfold(X_trn, y_trn,n_splits=5,max_depth=6,n_estimator=500, rand_st=3815):\n","  \"\"\"\n","  Perform training with XGBoost and evaluate in K-Fold cross-validation settings \n","  \"\"\"\n","  errors    = []\n","  precision = []\n","  recall    = []\n","  auc       = []\n","  kf = KFold(n_splits=n_splits, shuffle=True, random_state=3815)\n","\n","  for train_index, test_index in tqdm(kf.split(X_trn)):\n","     X_train_n, X_test_n = X_trn.values[train_index], X_trn.values[test_index]\n","     y_train_n, y_test_n = y_trn.values[train_index], y_trn.values[test_index]\n","\n","     model = XGBClassifier(\n","         max_depth=max_depth, n_estimators=n_estimator, random_state = rand_st\n","     )\n","     model.fit(X_train_n, y_train_n)\n","     y_pred = model.predict(X_test_n)\n","     accuracy = (sum(y_pred == y_test_n))/len(y_test_n)\n","     errors.append(1 - accuracy)\n","     precision.append(precision_score(y_test_n,y_pred))\n","     recall.append(recall_score(y_test_n,y_pred))\n","     auc.append(roc_auc_score(y_test_n,y_pred))\n","\n","  return model,errors, precision, recall, auc"],"metadata":{"id":"ZVDIO2ufmasC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pos_img_feats=pd.read_csv('/content/drive/MyDrive/685/emogen/evaluations/Vision_trans/pos_image_feats.csv')\n","# pos_text_feats=pd.read_csv('/content/drive/MyDrive/685/emogen/evaluations/Vision_trans/pos_text_feats.csv')\n","# non_pos_img_feats=pd.read_csv('/content/drive/MyDrive/685/emogen/evaluations/Vision_trans/non_pos_image_feats.csv')\n","# non_pos_text_feats=pd.read_csv('/content/drive/MyDrive/685/emogen/evaluations/Vision_trans/non_pos_text_feats.csv')\n","\n","\n","\n","\n","'''import pandas as pd\n","pos_img_feats=pd.read_csv('../emogen/evaluations/Vision_trans/pos_image_feats.csv')\n","pos_text_feats=pd.read_csv('../emogen/evaluations/Vision_trans/pos_text_feats.csv')\n","non_pos_img_feats=pd.read_csv('../emogen/evaluations/Vision_trans/non_pos_image_feats.csv')\n","non_pos_text_feats=pd.read_csv('../emogen/evaluations/Vision_trans/non_pos_text_feats.csv')'''"],"metadata":{"id":"GGgKpm_IqxYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_data=pd.concat([pos_img_feats,non_pos_img_feats],axis=0).sample(frac=1)\n","text_data=pd.concat([pos_text_feats,non_pos_text_feats],axis=0).sample(frac=1)\n","mmodal_data = img_data.merge(text_data,how='left',on='img_hash')"],"metadata":{"id":"vzh6BZxqmlp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.model_selection import train_test_split\n","# X_trn, X_tst, y_trn, y_tst = train_test_split(mmodal_data.drop(columns=['label','img_hash'],axis=1), mmodal_data['label'], test_size=0.15)\n","print(mmodal_data.shape, img_data.shape, text_data.shape)\n","print(img_data.columns)"],"metadata":{"id":"OHqe7wKzKFv2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639624382139,"user_tz":300,"elapsed":126,"user":{"displayName":"Himanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02796908766945347166"}},"outputId":"31434396-93fe-4a17-c015-e221304a1028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2000, 1539) (2000, 770) (3000, 770)\n","Index(['Unnamed: 0', 'imdim_0', 'imdim_1', 'imdim_2', 'imdim_3', 'imdim_4',\n","       'imdim_5', 'imdim_6', 'imdim_7', 'imdim_8',\n","       ...\n","       'imdim_759', 'imdim_760', 'imdim_761', 'imdim_762', 'imdim_763',\n","       'imdim_764', 'imdim_765', 'imdim_766', 'imdim_767', 'img_hash'],\n","      dtype='object', length=770)\n"]}]},{"cell_type":"code","source":["X_train = X_trn.values\n","y_train = y_trn.values\n","X_test  = X_tst.values\n","y_test  = y_tst.values \n","print(X_test.shape)"],"metadata":{"id":"Awblhnc2KUKp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639623392604,"user_tz":300,"elapsed":199,"user":{"displayName":"Himanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02796908766945347166"}},"outputId":"6c9ff319-1405-422e-b2c8-a56763ae8ef9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(300, 1537)\n"]}]},{"cell_type":"code","source":["model,errors, precision, recall, auc = xgb_train_kfold(X_trn,y_trn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifyfyc_yKkjf","executionInfo":{"status":"ok","timestamp":1639523186631,"user_tz":300,"elapsed":338293,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"22a49762-c9de-4e04-c91e-0554bf134486"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["5it [05:37, 67.59s/it]\n"]}]},{"cell_type":"code","source":["\n","'''labels = [int(x) for x in mmodal_data['label']]\n","mmodal_data['Target']  = labels'''\n","\n","print(\"The training errors on average is: \", np.round(np.mean(errors),4)*100)\n","print(\"The Precision on average is: \", np.round(np.mean(precision),4)*100)\n","print(\"The Recall on average is: \", np.round(np.mean(recall),4)*100)\n","print(\"The AUC Score on average is: \", np.round(np.mean(auc),4)*100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGn8rQfdmovy","executionInfo":{"status":"ok","timestamp":1639523186633,"user_tz":300,"elapsed":22,"user":{"displayName":"SOWMYA VASUKI JALLEPALLI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm48LrpyJtcWMlCPMTwv3NhzBw7ZUykumR45xiug=s64","userId":"14136760316602643415"}},"outputId":"c7c9d321-1fe4-492a-85c8-1ab508fa5f42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The training errors on average is:  27.71\n","The Precision on average is:  71.34\n","The Recall on average is:  75.03\n","The AUC Score on average is:  72.26\n"]}]},{"cell_type":"code","source":["pred_test = model.predict(X_test)\n","pred_prob = model.predict_proba(X_test)\n","accuracy = (sum(pred_test == y_test))*100/len(y_test)\n","precision = precision_score(y_test,pred_test)\n","recall    = recall_score(y_test,pred_test)\n","auc       = roc_auc_score(y_test,pred_test)\n","accuracy,precision,recall, auc"],"metadata":{"id":"htSXz9zdsVLj","executionInfo":{"status":"ok","timestamp":1639623351847,"user_tz":300,"elapsed":698,"user":{"displayName":"Himanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02796908766945347166"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"01c2be11-2dbc-400b-9fa7-8cb35f7fdde4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(93.66666666666667, 0.9266666666666666, 0.9455782312925171, 0.9368414032279578)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["import pickle\n","file_name = \"../emogen/evaluations/Vision_trans/xgb_cls_positive_vit_mmodal.pkl\"\n","\n","# save\n","# pickle.dump(model, open(file_name, \"wb\"))\n","\n","# load\n","model = pickle.load(open(file_name, \"rb\"))"],"metadata":{"id":"zjfaecS-Kr95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ePQ-uLtNKk52"},"execution_count":null,"outputs":[]}]}